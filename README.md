# autopodpiska_target_action
Predicts the intended user action based on their input device data and the source of their website visit

## Предсказания конверсии для сервиса Сбер Автоподписка

### Введение
Даны данные Google metrics (или аналогичной программы) по посещению сайта «СберАвтоподписка». Данные представляют собой 2 таблицы с общим ключом (‘session_id’). В одной таблице данные по действиям, которые совершил пользователь на сайте, в другом данные пользователя, устройства с которого он подключился и источника откуда он пришёл на сайт.  
Требуется очистить данные, обучить модель, которая будет предсказывать факт совершения пользователем целевого действия (оформление заявки на аренду авто, заявка на звонок менеджера и.т.п.), основываяь на данных из второй таблицы (характеристика устройства пользователя, и источник откуда он пришёл на сайт). 
Метрика модели – AUC >= 0.65. Это не очень большая точность, но в данном случае, когда существует большая неопределенность (желание пользователя арендовать авто), а также возможные цели применения такой модели, эта точность является приемлемой.  
Подготовка данных.
Объединение таблиц. В данном случае нам нужно выделить целевую переменную из таблицы «ga_hits.pkl». В условиях задания был дан словарь со значениями колонки event_action которые соответствуют целевому действию. Создаём новую колонку target в которой 1 будет если значение в event_action есть в словаре, и 0 если нет. Так как в таблице «ga_hits.pkl» у нас много колонок с одинаковым session_id (пользователь совершает множество действий на сайте, каждое – отдельная строка) – мы делаем сводную таблицу по значению session_id, и суммируем значение колонки target. Теперь если значение в этой колонке больше 1 меняем на 1, т.к. по условию задания для нас нет разницы одно или несколько целевых действий совершил пользователь. Теперь делаем merge с inner join сводной таблицы и таблицы «ga_session.pkl» по ключу session_id. 

### Удаление колонок. 
Удаляем колонки: ['session_id', 'client_id', 'visit_date', 'visit_time', 'device_model']. 
'session_id', 'client_id' – так как являются идентификаторами пользователя и сессии и не несут полезной информации для модели. 
'visit_date', 'visit_time' – исходя из поставленной задачи – модель не принимает эти значения.  Плюс могут дать не совсем корректный результат из-за неравномерного распределения по времени. Данные даны за 10 месяцев => модель не будет корректна в оставшиеся два месяца. Либо со временем популярность сервиса растёт из-за эффекта низкой базы – тогда со временем модель, выученная на этих данных, будет давать более положительные прогнозы. 
'device_model' – в этой колонке совсем нет данных – она не несёт полезной информации.
Заполнение пропусков. Была реализована стратегия в отношении пропусков – заполнять значением ‘other’, т.к. в большинстве случаев невозможно вычленить необходимые данные из других колонок. Однако есть 2 исключения «device_brand» и «device_os». В первом случае есть данные, которые формально не являются пустыми т.к. представляют собой строку – но без значений, в этом случае если тип устройства – desktop – записываем данные как “pc”. Колонку «device_os» заполняем с помощью кастомной функции. 
Удаление редких значений. Во многих фичах встречаются очень редкие значения это может стать проблемой при кодировании элементов – т.к. будет создано слишком большое пространство элементов. Поэтому при кодирование с помощью OneHotEncoder(), прописываем параметр min_frequency, который будет отсекать лишние значения. Он был установлен на уровне 0.0001. Тогда значения, которые встречаются реже чем в 0.01% случаев будут записаны в параметр infrequent. Handle_unknown выставляем infrequent_if_exist. 
Генерация категориальных фич. Согласно заданию, смотрим по ключам в utm_medium – является ли трафик органическим или нет. В utm_source также по данным ключам смотрим является ли трафик из социальных сетей или нет. 
Генерация численных фич. Данные по разрешению экрана устройства пользователя представлены в строковом виде (например «1920х1080»). Было создано 2 фичи – pixels представляющая общее количество пикселей – создана перемножением числа пикселей по ширине и по высоте монитора. Aspect_ratio  - значение соотношения сторон, получаем делением одного числа на другое. Устанавливаем границы нормальных значений в 4K – верхняя граница, «640х360» - нижняя граница разрешения экрана. Стандартизируем числинные значения с помощью MinMaxScaler().

### Выбор модели.
Исходя из объёма 1732266x857 и характера данных (данные очень разреженные), а также из поставленной задачи (Бинарная классификация) выбор падает на три возможных варианта модели: Logistic regression, SVM, MLP. По итогам первых тестов стало понятно, что размер данных все же слишком велик для SVM и для MLP (метод fit выполняется слишком долго). Оставляем Logistic regression. 
Выбор параметров модели. Так как наши классы несбалансированные – положительный класс встречается гораздо меньше, чем 0, выставляем параметр class_weight='balanced' – это поможет учесть эту особенность данных. Max_iter – выставляем на 10000, иначе вылазит ошибка. Далее с помощью GridSearch – варьируем параметр С в пределах 0.01 до 100. Параметр С – отвечает за сжимаемость к нулю коэффициентов в линейной модели. Чем больше С – тем более сложная и чувствительная получается модель. Решётчатый анализ показал, что лучшие результаты модели с параметром С=0.1


### Подготовка финального решения.

Проект выполнен в среде разработки Pycharm. Структура проекта представлена на рисунке ниже:

![image](https://github.com/Rost-Korol/autopodpiska_target_action/assets/91683515/4c16409e-c3f0-4bab-8c52-3e6f546d6c2b)

 
В основной директории проекта расположены директории с данными: 
•	data - начальные данные проекта плюс python код для соединения их в один исходный датафрейм
•	model – здесь хранится pipeline модели, а также в подпапку models сохраняются обученные модели или настроенные пайплайны
•	samples – образцы данных из объединённого датафрейма с известным таргетом
•	blind_samples -такие же образцы, но без таргета (используются при вызове predict через Postman)

Также в основной директории проекта находятся исполняемые python файлы:
•	get_samples.py – вспомогательный скрипт для получения образцов данных для проверки
•	logreg.py – скрипт для решеточного поиска наилучших параметров модели logistic regression
•	testing_samples.py – вспомогательный скрипт для оценки правильности образцов в директории samples
•	main.py – основной исполняемый сервером скрипт


Принцип работы. В model.py – создан пайплайн для обучения и кросс-валидационной проверке модели. Он берет данные из data/join_data.pkl. Сохраняет обученные папйплайны в model/models/ , откуда можно легко выбрать нужную модель или пайплайн. 
В main.py  с нахолятся функции которые могут быть исполнены сервером:
•	/status – возвращает “I’m OK” – индикатор работы сервера
•	/version – Возвращает Метаданные модели 
•	/predict – работает с методом POST request – принимает необработанные данные в формате, который соответствует данным в исходном датафрейме – возвращает предсказание модели. 
При вызове метода predict – для обученного пайплайна происходит обработка данных и затем predict  в  final estimator т.е. в нашей модели. 

Модель загружает данные в необработанном виде, т.к. такие данные не заполняются пользователем, а получаются автоматически из анализа его устройства. 

### Результаты
Значение метрики roc_auc при кросс-валидационной проверке – 0.686. 
Время отклика 65 – 100 мс для одного образца через API.
Пример вызова predict через API:

![image](https://github.com/Rost-Korol/autopodpiska_target_action/assets/91683515/e182f376-7623-4352-b1e5-f4699f0cadb4)
 
Проверка на случайных образцах из датафрейма. Было выбрано случайным образом 3 образца с положительным таргетом и 3 образца с отрицательным. Эти образцы проверены моделью, было получено:
•	2 истинно положительных предсказания
•	2 истинно отрицательных предсказания
•	1 ложноположительное предсказание
•	1 ложноотрицательное предсказание
